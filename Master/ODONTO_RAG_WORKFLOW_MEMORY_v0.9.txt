ODONTO-RAG WORKFLOW MEMORY (DEV HANDOFF) — v0.9
================================================

PURPOSE
-------
This file is used at the beginning of each new development chat to restore
full context, decisions, and workflow for the odontoiatric RAG system.
Update this file at the end of every chat before switching to a new one.

PROJECT GOAL
------------
Create a RAG-based system focused on DIDACTIC PRESENTATIONS that:
- Ingests odontoiatric documents (PDF, PPTX, DOCX; later VIDEO)
- Understands them semantically (RAG)
- Generates structured teaching material (slides, courses, decks)
- Target audience: students, clinicians, colleagues
- Output: teaching-oriented, source-backed slides (ECM / academic)

================================================
HIGH-LEVEL STATUS (as of v0.5)
================================================
✔ Canonical storage layout defined (GCS)
✔ Catalog DB schema redesigned and working (SQLite)
✔ Document/version registration pipeline working
✔ GCS writer for items/assets working (tested)
⏳ Document AI real extraction (NEXT STEP)
⏳ Qdrant indexing of new model (items/assets)
⏳ Runtime RAG + FastAPI + UI

================================================
CORE ARCHITECTURE (UPDATED — v0.5)
================================================
RAW CONTENT (PDF / PPTX / DOCX / VIDEO)
  → GCS RAW bucket
  → extractor (Document AI / others)
  → GCS PARSED bucket (canonical layout)
      - items.jsonl
      - assets.jsonl
      - assets/images/
      - assets/tables/
      - raw/extractor_output.json
  → SQLite catalog (metadata only)
  → Qdrant (vector search)
  → RAG runtime
  → Slide / course generator
  → PPTX builder
  → UI

================================================
A. STORAGE (CANONICAL — v0.5)
================================================
Google Cloud Storage (GCS) is the SOURCE OF TRUTH.

RAW bucket:
- odonto-raw-docs-matcler/
- Original source files only (immutable)

PARSED bucket:
- odonto-parsed-matcler/

Canonical layout (per doc + version):
parsed/<doc_id>/<version_id>/
  items.jsonl
  assets.jsonl
  assets/images/
  assets/tables/
  raw/extractor_output.json

Design rules:
- items/assets are NEVER stored in SQLite
- GCS paths are deterministic and versioned
- Safe to re-ingest without data loss

================================================
B. CANONICAL DATA MODEL (NEW — v0.5)
================================================

1) items.jsonl (NDJSON)
- One record = one semantic teaching unit
- Used for embeddings + retrieval

Core fields:
- item_id
- doc_id
- version_id
- doc_type
- item_type
- text
- section / title
- locator (page, slide, time, bbox)
- source (uri, publisher, year)
- tags
- meta

2) assets.jsonl (NDJSON)
- One record = one non-text asset

Core fields:
- asset_id
- doc_id
- version_id
- asset_type
- caption
- locator (page, slide, time, bbox)
- files (image_uri, table_uri)
- table (optional)
- tags
- meta

================================================
C. CATALOG DATABASE (STABLE — v0.5)
================================================
Type:
- SQLite (local-first, admin/query only)

Purpose:
- Track documents and versions
- Drive ingest, query, UI
- NEVER store full text

Tables:
1) documents
2) document_versions

================================================
D. IMPLEMENTED DEV SCRIPTS (NEW — v0.5)
================================================

1) scripts/dev_register_doc_version.py
- Registers doc + version
- Computes canonical GCS paths

2) scripts/dev_write_sample_extract.py
- Writes items.jsonl / assets.jsonl to GCS
- Updates DB counters
- Uses gsutil

================================================
E. ENVIRONMENT (REFERENCE)
================================================
.env.local variables include:
- GCP_PROJECT=odontology-rag-slides
- GCP_LOCATION=europe-west1
- DOCAI_LOCATION=eu
- RAW_BUCKET=odonto-raw-docs-matcler
- PARSED_BUCKET=odonto-parsed-matcler
- RAW_PREFIX=test/articles/
- SQLITE_PATH=catalog.sqlite3
- QDRANT_URL=http://localhost:6333
- VERTEX_EMBED_MODEL=text-embedding-004

================================================
F. VERSIONING STRATEGY
================================================
- New version for every ingest logic change
- documents.active_version defines production view
- Old versions kept for audit

================================================
G. NEXT STEP
================================================
STEP 4 — REAL PDF INGEST WITH DOCUMENT AI LAYOUT PARSER
- Use Layout Parser processor
- Generate real items.jsonl / assets.jsonl
- Update DB counts and status
- Preserve raw extractor output

================================================
UPDATE LOG — v0.6 (APPEND-ONLY)
================================================
Date: 2026-02-06 (Europe/Rome)

What we did since v0.5
----------------------
1) Git/GitHub versioning
- Initialized / prepared the project for GitHub versioning.
- Confirmed .gitignore strategy: keep repo source-only; ignore runtime/state:
  - .env.local, *.sqlite3, qdrant/, qdrant.tar.gz, data/, storage/, out/, outputs/
- Confirmed that Qdrant (local index/state) must NOT be committed; only code + schemas + scripts are versioned.
- Confirmed that templates PPTX used for early tests were removed from the repo (tracked as deletions):
  - templates/chairside_test_template.pptx
  - templates/dark_master.pptx
  Rationale: templates are not part of the ingest/RAG foundation; reintroduce later with a dedicated commit and stable template strategy.

2) Document AI Layout ingest — moved from “sample extract” to real extractor script
- Added a real Document AI Layout Parser ingest script (new file):
  - scripts/dev_docai_layout_ingest.py
- Converted the script from gcsfs-based I/O to gsutil-based I/O immediately (robustness + consistency with prior work):
  - Read PDF bytes: gsutil cat gs://...
  - Write JSON/NDJSON outputs: gsutil cp - gs://...
- The script:
  - Reads documents.gcs_raw_path (or --raw-uri override)
  - Calls Document AI Layout Parser (ProcessDocument, raw_document)
  - Writes raw extractor dump to:
      parsed/<doc_id>/<version_id>/raw/extractor_output.json
    (path generated via ParsedLayout + gcs_uri)
  - Writes:
      items.jsonl  → one item per page (item_type="page")
      assets.jsonl → table assets (asset_type="table") + per-table JSON payload saved under assets/tables/<asset_id>.json
  - Updates SQLite document_versions:
      n_items, n_assets, ingest_status="ingested"

3) Requirements updated for STEP 4
- requirements.txt (root) now includes only what is needed for this step:
  - google-cloud-documentai
  - python-pptx
- Removed gcsfs dependency after switching to gsutil.

Files created/updated in this step
----------------------------------
- requirements.txt (new/updated)
- scripts/dev_docai_layout_ingest.py (new) — gsutil-based Document AI Layout ingest
- (Repo hygiene) .gitignore updated/validated to keep runtime out of git

Notes / Constraints
-------------------
- The gsutil approach assumes Google Cloud SDK is installed and authenticated on the dev machine.
- Document AI credentials and target processor are provided via:
  - --project / --location / --processor-id
  and/or env:
  - GCP_PROJECT, DOCAI_LOCATION, DOCAI_PROCESSOR_ID
- Current extraction scope:
  - Items: page-level text units
  - Assets: tables only
  - Figures/images extraction is planned next (from Document AI page image / rendered pages / figure detection strategy).

================================================
NEXT STEP (v0.6 → v0.7 plan)
================================================
STEP 5 — Run real DocAI ingest end-to-end on the test PDFs + validate outputs
1) Ensure DB registration exists for doc_id + version_id (document_versions row has items_uri/assets_uri)
2) Run:
   python scripts/dev_docai_layout_ingest.py --doc-id <...> --version-id v2-docai --processor-id <...>
3) Validate in GCS:
   - items.jsonl exists and has expected number of lines (≈ number of pages)
   - assets.jsonl exists and contains table assets if tables are detected
   - raw/extractor_output.json exists
   - assets/tables/<asset_id>.json exists for each detected table
4) Decide chunking granularity for items:
   - page-level vs paragraph/section-level items for better retrieval
   - update script accordingly (still NDJSON, but item_type changes: paragraph/section)
5) After content is stable:
   - Add Qdrant indexing for items (and optionally assets captions/metadata)
   - Establish collection naming convention including version + embed model
   - Implement runtime query path (RAG): embed query → Qdrant search → assemble context → LLM answer

================================================
UPDATE LOG — v0.7 (APPEND-ONLY)
================================================
Date: 2026-02-09

What we did since v0.6
----------------------
1) Script fixes for Document AI ingest
- Updated _text_from_anchor() to treat missing end_index as end=len(full_text).
- Added debug prints after Document AI returns the document:
  - pdf_bytes_len
  - pages_count
  - text_len

2) Dependencies installed (local)
- Installed runtime deps needed to run the ingest:
  - google-cloud-documentai, python-pptx, gcsfs (from requirements.txt)
  - sqlalchemy (missing from requirements.txt but required by catalog DB)

3) Ran real DocAI ingest (article-adhesive-systems, v1-docai)
- Command used (with env):
  GCP_PROJECT=odontology-rag-slides DOCAI_LOCATION=eu PYTHONPATH=src \
  python3 scripts/dev_docai_layout_ingest.py \
    --doc-id article-adhesive-systems \
    --version-id v1-docai \
    --processor-id cefc5e7bf97d1e2c
- Debug output:
  - pdf_bytes_len: 1627148
  - pages_count: 0
  - text_len: 0
- Script completed and wrote GCS outputs, but no items/assets were produced.

4) GCS validation
- Parsed outputs present:
  - items.jsonl (size 1 byte; empty)
  - assets.jsonl (size 1 byte; empty)
  - raw/extractor_output.json (size ~450 KB)
- No assets/tables/*.json files were created (no tables detected).

Learnings / anomalies
---------------------
- Document AI returned zero pages and empty text for this PDF, so items.jsonl is empty.
- This fails the “non-trivial items.jsonl with JSON lines” requirement.

NEXT STEP
---------
1) Inspect raw/extractor_output.json to see why pages/text are missing.
2) Verify processor_id and DOCAI_LOCATION are correct for the Layout processor.
3) Re-run ingest after confirmation, then re-validate items.jsonl/ assets.jsonl.

--- Addendum (2026-02-09) ---
Follow-up fixes + rerun
-----------------------
1) Ingest script update (layout parsing)
- Added recursive traversal of document_layout blocks to capture nested text and table blocks.
- Added table extraction for layout table_block rows.

2) Re-ran ingest (article-adhesive-systems, v1-docai)
- Debug output:
  - pdf_bytes_len: 1627148
  - pages_count: 0
  - text_len: 0
- Output counts:
  - items: 395
  - assets: 7

3) GCS validation (non-trivial outputs now present)
- items.jsonl size: 181,057 bytes (contains JSON lines)
- assets.jsonl size: 2,304 bytes
- assets/tables/*.json: 7 files

UPDATE LOG — v0.9 (APPEND-ONLY)
================================
Date: 2026-02-11 (Europe/Rome)

Architecture recap (short)
--------------------------
- FastAPI app in src/odonto_rag/api/rag_app.py with /healthz, /rag/query, /rag/answer.
- Qdrant collections named odonto_items__{version}__{embed_model}.
- Vertex AI embeddings via REST (text-embedding-004) and LLM via vertexai GenerativeModel.
- Catalog metadata in SQLite (catalog.sqlite3) with GCS-backed items.jsonl.

STEP 7 implemented
------------------
- Added retrieval helper used by /rag/query and new /rag/answer endpoint.
- /rag/answer builds context with [S#] tokens + page ranges and generates didactic answers with citations.
- Structured citations extracted from [S#] tokens and mapped to page_start/page_end.
- Qdrant dev index/query scripts updated for consistent collection naming, safe upsert IDs,
  payload page_start/page_end, default QDRANT_URL, and small retry/backoff.

Env vars needed
---------------
- QDRANT_URL (default http://localhost:6333)
- QDRANT_API_KEY (optional)
- GCP_PROJECT or PROJECT_ID
- GCP_LOCATION or LOCATION
- VERTEX_EMBED_MODEL (default text-embedding-004)
- VERTEX_LLM_MODEL
- SQLITE_PATH (for dev_index_items_qdrant.py; default catalog.sqlite3)

Next step
---------
- Run a local /rag/answer smoke test and confirm citations map to correct pages; then add lightweight
  evaluation prompts for coverage and grounding.

Addendum — 2026-02-11
----------------------
- LLM model availability may differ by region; embeddings can stay in europe-west1 while LLM may require us-central1.
- New env vars for split locations:
  - VERTEX_EMBED_LOCATION (defaults to GCP_LOCATION/LOCATION)
  - VERTEX_LLM_LOCATION (defaults to GCP_LOCATION/LOCATION)
- Recommended dev smoke-test LLM model: gemini-2.0-flash-001 in us-central1 (or any available Gemini model with -001/-002 suffix).

Addendum — 2026-02-11 (STEP 8 MVP)
----------------------------------
- Added new POST endpoint `/rag/outline` in `src/odonto_rag/api/rag_app.py`.
- Request model: `RagOutlineRequest` with `query/topic`, `top_k` (default 25), optional `doc_id`, required `version`, `max_sections` (default 10), and `include_retrieved` (default false).
- Retrieval path reuses existing `retrieve_items(query, top_k, doc_id, version)` and builds `[S#]` context snippets (about 500 chars each).
- Added lightweight dedup before context assembly: repeated `header` items are collapsed by case-insensitive identical text.
- LLM prompt requires strict JSON-only outline output; parse failures return HTTP 500 including the first 200 chars of model output.
- Response model: `RagOutlineResponse` with `title`, structured `sections`, resolved `citations` (`RagCitation` mapped from `S#` tokens to item/page ranges), and optional `retrieved` payload.
- Env vars reused (no new env required): `GCP_PROJECT`/`PROJECT_ID`, `VERTEX_LLM_LOCATION` (or `GCP_LOCATION`/`LOCATION` fallback), `VERTEX_LLM_MODEL`, plus retrieval-side vars (`VERTEX_EMBED_LOCATION`, `VERTEX_EMBED_MODEL`, `QDRANT_URL`, `QDRANT_API_KEY`).
- outline parser now strips ```json fences

UPDATE LOG — v0.10 (APPEND-ONLY)
=================================
Date: 2026-02-11

STEP 9 implemented (slide planning)
-----------------------------------
- Added new POST endpoint `/rag/slides/plan` in `src/odonto_rag/api/rag_app.py`.
- Request supports two paths:
  - `outline` provided: plans slides from the given outline JSON.
  - `query` provided (without outline): generates outline by reusing `/rag/outline` logic, then plans slides.
- Added strict JSON parsing with fenced-output cleanup for slide-plan LLM output.
- Added lightweight dedup:
  - outline section heading dedup before planning
  - slide title dedup after generation
- Added slide source mapping from citation tokens (`S#`) to `RagCitation` with `page_start/page_end`.
- Response returns `slides` and, on query path, `outline_used`; optional `retrieved` respected via `include_retrieved`.

Files touched
-------------
- src/odonto_rag/api/rag_app.py
- Master/ODONTO_RAG_WORKFLOW_MEMORY_v0.9.txt

How to test
-----------
- Start API: `uvicorn odonto_rag.api.rag_app:app --reload --port 8000`
- Smoke test query path: POST `/rag/slides/plan` with `{query, version, ...}`
- Smoke test outline path: POST `/rag/slides/plan` with `{outline, version, ...}`

Next step
---------
- STEP 10: PPTX generation from slide plan output.

v0.10 (hardening)
-----------------
Cosa aggiunto
- Validator XOR su `RagSlidesPlanRequest` (`outline` XOR `query`):
  - errore chiaro se mancano entrambi: "Provide either 'outline' or 'query'"
  - errore chiaro se presenti entrambi: "Provide only one of 'outline' or 'query', not both"
- `max_sections` reso opzionale in `RagSlidesPlanRequest` (default `None`), con default operativo `10` solo nel path che genera outline internamente.
- Guardrail output slide: se dopo parsing+dedup risultano 0 slide, ritorna HTTP 500 con detail esplicito.
- Prompt input per LLM aggiornato con `json.dumps(..., ensure_ascii=False)`.

Files touched
- src/odonto_rag/api/rag_app.py
- Master/ODONTO_RAG_WORKFLOW_MEMORY_v0.9.txt

Comandi test
- python -m compileall src/odonto_rag/api/rag_app.py
- uvicorn odonto_rag.api.rag_app:app --port 8010
- curl POST /rag/slides/plan (query path)
- curl POST /rag/slides/plan (outline path)
- v0.10 hardening fix XOR: `/rag/slides/plan` now returns HTTP 400 when both `outline` and non-empty `query` are provided.
