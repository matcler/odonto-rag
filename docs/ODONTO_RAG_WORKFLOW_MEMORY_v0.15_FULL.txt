ODONTO-RAG WORKFLOW MEMORY (DEV HANDOFF) — v0.9
================================================

PURPOSE
-------
This file is used at the beginning of each new development chat to restore
full context, decisions, and workflow for the odontoiatric RAG system.
Update this file at the end of every chat before switching to a new one.

PROJECT GOAL
------------
Create a RAG-based system focused on DIDACTIC PRESENTATIONS that:
- Ingests odontoiatric documents (PDF, PPTX, DOCX; later VIDEO)
- Understands them semantically (RAG)
- Generates structured teaching material (slides, courses, decks)
- Target audience: students, clinicians, colleagues
- Output: teaching-oriented, source-backed slides (ECM / academic)

================================================
HIGH-LEVEL STATUS (as of v0.5)
================================================
✔ Canonical storage layout defined (GCS)
✔ Catalog DB schema redesigned and working (SQLite)
✔ Document/version registration pipeline working
✔ GCS writer for items/assets working (tested)
⏳ Document AI real extraction (NEXT STEP)
⏳ Qdrant indexing of new model (items/assets)
⏳ Runtime RAG + FastAPI + UI

================================================
CORE ARCHITECTURE (UPDATED — v0.5)
================================================
RAW CONTENT (PDF / PPTX / DOCX / VIDEO)
  → GCS RAW bucket
  → extractor (Document AI / others)
  → GCS PARSED bucket (canonical layout)
      - items.jsonl
      - assets.jsonl
      - assets/images/
      - assets/tables/
      - raw/extractor_output.json
  → SQLite catalog (metadata only)
  → Qdrant (vector search)
  → RAG runtime
  → Slide / course generator
  → PPTX builder
  → UI

================================================
A. STORAGE (CANONICAL — v0.5)
================================================
Google Cloud Storage (GCS) is the SOURCE OF TRUTH.

RAW bucket:
- odonto-raw-docs-matcler/
- Original source files only (immutable)

PARSED bucket:
- odonto-parsed-matcler/

Canonical layout (per doc + version):
parsed/<doc_id>/<version_id>/
  items.jsonl
  assets.jsonl
  assets/images/
  assets/tables/
  raw/extractor_output.json

Design rules:
- items/assets are NEVER stored in SQLite
- GCS paths are deterministic and versioned
- Safe to re-ingest without data loss

================================================
B. CANONICAL DATA MODEL (NEW — v0.5)
================================================

1) items.jsonl (NDJSON)
- One record = one semantic teaching unit
- Used for embeddings + retrieval

Core fields:
- item_id
- doc_id
- version_id
- doc_type
- item_type
- text
- section / title
- locator (page, slide, time, bbox)
- source (uri, publisher, year)
- tags
- meta

2) assets.jsonl (NDJSON)
- One record = one non-text asset

Core fields:
- asset_id
- doc_id
- version_id
- asset_type
- caption
- locator (page, slide, time, bbox)
- files (image_uri, table_uri)
- table (optional)
- tags
- meta

================================================
C. CATALOG DATABASE (STABLE — v0.5)
================================================
Type:
- SQLite (local-first, admin/query only)

Purpose:
- Track documents and versions
- Drive ingest, query, UI
- NEVER store full text

Tables:
1) documents
2) document_versions

================================================
D. IMPLEMENTED DEV SCRIPTS (NEW — v0.5)
================================================

1) scripts/dev_register_doc_version.py
- Registers doc + version
- Computes canonical GCS paths

2) scripts/dev_write_sample_extract.py
- Writes items.jsonl / assets.jsonl to GCS
- Updates DB counters
- Uses gsutil

================================================
E. ENVIRONMENT (REFERENCE)
================================================
.env.local variables include:
- GCP_PROJECT=odontology-rag-slides
- GCP_LOCATION=europe-west1
- DOCAI_LOCATION=eu
- RAW_BUCKET=odonto-raw-docs-matcler
- PARSED_BUCKET=odonto-parsed-matcler
- RAW_PREFIX=test/articles/
- SQLITE_PATH=catalog.sqlite3
- QDRANT_URL=http://localhost:6333
- VERTEX_EMBED_MODEL=text-embedding-004

================================================
F. VERSIONING STRATEGY
================================================
- New version for every ingest logic change
- documents.active_version defines production view
- Old versions kept for audit

================================================
G. NEXT STEP
================================================
STEP 4 — REAL PDF INGEST WITH DOCUMENT AI LAYOUT PARSER
- Use Layout Parser processor
- Generate real items.jsonl / assets.jsonl
- Update DB counts and status
- Preserve raw extractor output

================================================
UPDATE LOG — v0.6 (APPEND-ONLY)
================================================
Date: 2026-02-06 (Europe/Rome)

What we did since v0.5
----------------------
1) Git/GitHub versioning
- Initialized / prepared the project for GitHub versioning.
- Confirmed .gitignore strategy: keep repo source-only; ignore runtime/state:
  - .env.local, *.sqlite3, qdrant/, qdrant.tar.gz, data/, storage/, out/, outputs/
- Confirmed that Qdrant (local index/state) must NOT be committed; only code + schemas + scripts are versioned.
- Confirmed that templates PPTX used for early tests were removed from the repo (tracked as deletions):
  - templates/chairside_test_template.pptx
  - templates/dark_master.pptx
  Rationale: templates are not part of the ingest/RAG foundation; reintroduce later with a dedicated commit and stable template strategy.

2) Document AI Layout ingest — moved from “sample extract” to real extractor script
- Added a real Document AI Layout Parser ingest script (new file):
  - scripts/dev_docai_layout_ingest.py
- Converted the script from gcsfs-based I/O to gsutil-based I/O immediately (robustness + consistency with prior work):
  - Read PDF bytes: gsutil cat gs://...
  - Write JSON/NDJSON outputs: gsutil cp - gs://...
- The script:
  - Reads documents.gcs_raw_path (or --raw-uri override)
  - Calls Document AI Layout Parser (ProcessDocument, raw_document)
  - Writes raw extractor dump to:
      parsed/<doc_id>/<version_id>/raw/extractor_output.json
    (path generated via ParsedLayout + gcs_uri)
  - Writes:
      items.jsonl  → one item per page (item_type="page")
      assets.jsonl → table assets (asset_type="table") + per-table JSON payload saved under assets/tables/<asset_id>.json
  - Updates SQLite document_versions:
      n_items, n_assets, ingest_status="ingested"

3) Requirements updated for STEP 4
- requirements.txt (root) now includes only what is needed for this step:
  - google-cloud-documentai
  - python-pptx
- Removed gcsfs dependency after switching to gsutil.

Files created/updated in this step
----------------------------------
- requirements.txt (new/updated)
- scripts/dev_docai_layout_ingest.py (new) — gsutil-based Document AI Layout ingest
- (Repo hygiene) .gitignore updated/validated to keep runtime out of git

Notes / Constraints
-------------------
- The gsutil approach assumes Google Cloud SDK is installed and authenticated on the dev machine.
- Document AI credentials and target processor are provided via:
  - --project / --location / --processor-id
  and/or env:
  - GCP_PROJECT, DOCAI_LOCATION, DOCAI_PROCESSOR_ID
- Current extraction scope:
  - Items: page-level text units
  - Assets: tables only
  - Figures/images extraction is planned next (from Document AI page image / rendered pages / figure detection strategy).

================================================
NEXT STEP (v0.6 → v0.7 plan)
================================================
STEP 5 — Run real DocAI ingest end-to-end on the test PDFs + validate outputs
1) Ensure DB registration exists for doc_id + version_id (document_versions row has items_uri/assets_uri)
2) Run:
   python scripts/dev_docai_layout_ingest.py --doc-id <...> --version-id v2-docai --processor-id <...>
3) Validate in GCS:
   - items.jsonl exists and has expected number of lines (≈ number of pages)
   - assets.jsonl exists and contains table assets if tables are detected
   - raw/extractor_output.json exists
   - assets/tables/<asset_id>.json exists for each detected table
4) Decide chunking granularity for items:
   - page-level vs paragraph/section-level items for better retrieval
   - update script accordingly (still NDJSON, but item_type changes: paragraph/section)
5) After content is stable:
   - Add Qdrant indexing for items (and optionally assets captions/metadata)
   - Establish collection naming convention including version + embed model
   - Implement runtime query path (RAG): embed query → Qdrant search → assemble context → LLM answer

================================================
UPDATE LOG — v0.7 (APPEND-ONLY)
================================================
Date: 2026-02-09

What we did since v0.6
----------------------
1) Script fixes for Document AI ingest
- Updated _text_from_anchor() to treat missing end_index as end=len(full_text).
- Added debug prints after Document AI returns the document:
  - pdf_bytes_len
  - pages_count
  - text_len

2) Dependencies installed (local)
- Installed runtime deps needed to run the ingest:
  - google-cloud-documentai, python-pptx, gcsfs (from requirements.txt)
  - sqlalchemy (missing from requirements.txt but required by catalog DB)

3) Ran real DocAI ingest (article-adhesive-systems, v1-docai)
- Command used (with env):
  GCP_PROJECT=odontology-rag-slides DOCAI_LOCATION=eu PYTHONPATH=src \
  python3 scripts/dev_docai_layout_ingest.py \
    --doc-id article-adhesive-systems \
    --version-id v1-docai \
    --processor-id cefc5e7bf97d1e2c
- Debug output:
  - pdf_bytes_len: 1627148
  - pages_count: 0
  - text_len: 0
- Script completed and wrote GCS outputs, but no items/assets were produced.

4) GCS validation
- Parsed outputs present:
  - items.jsonl (size 1 byte; empty)
  - assets.jsonl (size 1 byte; empty)
  - raw/extractor_output.json (size ~450 KB)
- No assets/tables/*.json files were created (no tables detected).

Learnings / anomalies
---------------------
- Document AI returned zero pages and empty text for this PDF, so items.jsonl is empty.
- This fails the “non-trivial items.jsonl with JSON lines” requirement.

NEXT STEP
---------
1) Inspect raw/extractor_output.json to see why pages/text are missing.
2) Verify processor_id and DOCAI_LOCATION are correct for the Layout processor.
3) Re-run ingest after confirmation, then re-validate items.jsonl/ assets.jsonl.

--- Addendum (2026-02-09) ---
Follow-up fixes + rerun
-----------------------
1) Ingest script update (layout parsing)
- Added recursive traversal of document_layout blocks to capture nested text and table blocks.
- Added table extraction for layout table_block rows.

2) Re-ran ingest (article-adhesive-systems, v1-docai)
- Debug output:
  - pdf_bytes_len: 1627148
  - pages_count: 0
  - text_len: 0
- Output counts:
  - items: 395
  - assets: 7

3) GCS validation (non-trivial outputs now present)
- items.jsonl size: 181,057 bytes (contains JSON lines)
- assets.jsonl size: 2,304 bytes
- assets/tables/*.json: 7 files

UPDATE LOG — v0.9 (APPEND-ONLY)
================================
Date: 2026-02-11 (Europe/Rome)

Architecture recap (short)
--------------------------
- FastAPI app in src/odonto_rag/api/rag_app.py with /healthz, /rag/query, /rag/answer.
- Qdrant collections named odonto_items__{version}__{embed_model}.
- Vertex AI embeddings via REST (text-embedding-004) and LLM via vertexai GenerativeModel.
- Catalog metadata in SQLite (catalog.sqlite3) with GCS-backed items.jsonl.

STEP 7 implemented
------------------
- Added retrieval helper used by /rag/query and new /rag/answer endpoint.
- /rag/answer builds context with [S#] tokens + page ranges and generates didactic answers with citations.
- Structured citations extracted from [S#] tokens and mapped to page_start/page_end.
- Qdrant dev index/query scripts updated for consistent collection naming, safe upsert IDs,
  payload page_start/page_end, default QDRANT_URL, and small retry/backoff.

Env vars needed
---------------
- QDRANT_URL (default http://localhost:6333)
- QDRANT_API_KEY (optional)
- GCP_PROJECT or PROJECT_ID
- GCP_LOCATION or LOCATION
- VERTEX_EMBED_MODEL (default text-embedding-004)
- VERTEX_LLM_MODEL
- SQLITE_PATH (for dev_index_items_qdrant.py; default catalog.sqlite3)

Next step
---------
- Run a local /rag/answer smoke test and confirm citations map to correct pages; then add lightweight
  evaluation prompts for coverage and grounding.

Addendum — 2026-02-11
----------------------
- LLM model availability may differ by region; embeddings can stay in europe-west1 while LLM may require us-central1.
- New env vars for split locations:
  - VERTEX_EMBED_LOCATION (defaults to GCP_LOCATION/LOCATION)
  - VERTEX_LLM_LOCATION (defaults to GCP_LOCATION/LOCATION)
- Recommended dev smoke-test LLM model: gemini-2.0-flash-001 in us-central1 (or any available Gemini model with -001/-002 suffix).

Addendum — 2026-02-11 (STEP 8 MVP)
----------------------------------
- Added new POST endpoint `/rag/outline` in `src/odonto_rag/api/rag_app.py`.
- Request model: `RagOutlineRequest` with `query/topic`, `top_k` (default 25), optional `doc_id`, required `version`, `max_sections` (default 10), and `include_retrieved` (default false).
- Retrieval path reuses existing `retrieve_items(query, top_k, doc_id, version)` and builds `[S#]` context snippets (about 500 chars each).
- Added lightweight dedup before context assembly: repeated `header` items are collapsed by case-insensitive identical text.
- LLM prompt requires strict JSON-only outline output; parse failures return HTTP 500 including the first 200 chars of model output.
- Response model: `RagOutlineResponse` with `title`, structured `sections`, resolved `citations` (`RagCitation` mapped from `S#` tokens to item/page ranges), and optional `retrieved` payload.
- Env vars reused (no new env required): `GCP_PROJECT`/`PROJECT_ID`, `VERTEX_LLM_LOCATION` (or `GCP_LOCATION`/`LOCATION` fallback), `VERTEX_LLM_MODEL`, plus retrieval-side vars (`VERTEX_EMBED_LOCATION`, `VERTEX_EMBED_MODEL`, `QDRANT_URL`, `QDRANT_API_KEY`).
- outline parser now strips ```json fences

UPDATE LOG — v0.10 (APPEND-ONLY)
=================================
Date: 2026-02-12

STEP 9 hardening completed (XOR + robustness)
---------------------------------------------
- Endpoint `/rag/slides/plan` hardened:
  - Enforced XOR between `outline` and `query` at route level:
    - If both provided -> HTTP 400 with detail: "Provide only one of outline or query, not both"
    - If neither provided -> HTTP 400 with detail: "Provide either 'outline' or 'query'"
  - Keeps successful paths unchanged (query-only and outline-only return HTTP 200 with slide plan JSON).
- Added more robust JSON cleanup for LLM outputs:
  - `_strip_json_fences` now handles BOM and extracts fenced ```json blocks via regex before parsing.
- Added logging for outline parse failures:
  - Logs first ~500 chars of the raw model output when JSON parse fails to aid debugging.
- Note: `/rag/slides/plan` (query path) depends on retrieval; if Qdrant is not reachable (e.g., Docker daemon stopped),
  the endpoint can error. Ensure Qdrant is running and `QDRANT_URL=http://localhost:6333`.

Smoke tests (local)
-------------------
- Health:
  - `curl -s http://localhost:8000/healthz | jq`
- Slides plan (query-only) -> 200:
  - `curl -s -o /dev/null -w "HTTP %{http_code}\n" -X POST http://localhost:8000/rag/slides/plan -H "Content-Type: application/json" -d '{"query":"Adhesive systems","version":"v1-docai"}'`
- Slides plan (missing both) -> 400:
  - `curl -s -o /dev/null -w "HTTP %{http_code}\n" -X POST http://localhost:8000/rag/slides/plan -H "Content-Type: application/json" -d '{"version":"v1-docai"}'`
- Slides plan (outline + query) -> 400 with detail:
  - `curl -s -X POST http://localhost:8000/rag/slides/plan -H "Content-Type: application/json" -d '{"outline":{...},"query":"x","version":"v1-docai"}' | jq`

Files touched
-------------
- src/odonto_rag/api/rag_app.py
- Master/ODONTO_RAG_WORKFLOW_MEMORY_v0.9.txt

Next step
---------
- STEP 10: PPTX generation from slide plan output.

================================================
UPDATE LOG — v0.12 (APPEND-ONLY)
================================================
Date: 2026-02-12

Scope note (chat scoping)
-------------------------
- This chat is considered STEP 10 only.
- Any auxiliary improvements made while delivering STEP 10 (e.g., citation formatting refinements) are treated as STEP 10 implementation details, not a separate milestone in this workflow log.

STEP 10 — PPTX generation from slide plan (DONE)
------------------------------------------------
Goal
- Generate a .pptx deck from the strict JSON slide plan output of POST /rag/slides/plan.

Delivered
- POST /rag/slides/pptx added to FastAPI runtime (local-first).
- PPTX generation implemented with python-pptx under src/odonto_rag/deck/.
- Deterministic output:
  - stable slide order and rendering (title + bullets),
  - stable filename generation (same slide_plan -> same output filename/path),
  - output directory configurable via env (PPTX_OUTPUT_DIR).
- Citations preserved:
  - slide footer contains citation tokens (S#) and optional document-level citation string,
  - speaker notes include a “Sources” section mapping S# to page ranges (and other available identifiers).

Smoke tests (validated)
- Generate slide plan -> generate PPTX -> verify file exists and is non-empty.
- Optional: unzip deck.pptx to confirm ppt/slides/*.xml and ppt/notesSlides/*.xml presence.

STEP 11 — Figures & tables in PPTX (NEXT)
-----------------------------------------
Goal
- Include figures/tables from source PDFs into generated PPTX slides (initially minimal and deterministic).

First check
- Verify ingest/layout pipeline already captures figure/table blocks and bounding boxes (Document AI layout -> canonical layout).
- Confirm where figure/table metadata is stored (layout JSON, catalog, Qdrant payload) and how to map it to doc_id + page.

Planned approach (minimal)
- During PPTX generation, optionally add one visual per slide:
  - Use slide.sources[].doc_id + page_start/page_end to search for candidate FIGURE/TABLE blocks in the canonical layout.
  - Select deterministically (largest area or first in reading order).
  - Render PDF page to image and crop by bbox; embed image on slide.
- Keep citations and speaker notes behavior unchanged.

Test set expansion (for STEP 11)
- Prepare 2–3 articles (PDFs) that include both tables and figures.
- Ensure ingest produces layout artifacts for these PDFs and that RAG retrieval can cite pages containing visuals.

================================================
UPDATE LOG — v0.13 (APPEND-ONLY)
================================================
Date: 2026-02-12 (Europe/Rome)

STEP S1 — Macro-specialty segmentation (DESIGN + IMPLEMENTATION PLAN)
--------------------------------------------------------------------
Goal
- Support multi-specialty corpora by grouping documents by macro-specialty (e.g., endodonzia, implantologia, conservativa).
- Enable retrieval filtering by specialty.
- Ensure slide plans + PPTX generation can target a single specialty without cross-specialty bleed.

Design decision (storage model)
- Canonical source: documents.metadata_json["specialty"].
  - Current representation: a single string (e.g., "endodonzia").
  - Forward-compatible: allow list[str] ("specialties") later; retrieval filter will accept a single specialty value for now.
- Retrieval/index surface: each Qdrant chunk payload MUST include:
  - payload["specialty"] = "<specialty-string>"
  so filtering happens at vector search time.

Derivation rules (minimal + explicit)
- Primary: explicit CLI parameter during registration/ingest (recommended).
- Fallback: derive from GCS raw path convention:
    gs://<RAW_BUCKET>/<RAW_PREFIX>/<specialty>/...
  Example:
    raw/endodonzia/<file>.pdf  -> specialty="endodonzia"
- If neither available, specialty is omitted (treated as “global corpus” / untagged).

Implementation plan (surgical changes)
1) Catalog metadata persistence
   - Extend scripts/dev_register_doc_version.py:
     - add optional --specialty (string)
     - if missing, derive from --raw-uri (first folder after RAW_PREFIX if present)
     - upsert documents.metadata_json["specialty"] (preserve existing metadata keys such as "citation")
   - (Optional) add a small helper script scripts/catalog_set_specialty.py mirroring catalog_set_citation.py:
     - set specialty for an existing doc_id without re-registration.

2) Qdrant indexing payload extension
   - Update scripts/dev_index_items_qdrant.py:
     - read documents.metadata_json["specialty"] for the target doc_id once (SQLite)
     - add payload["specialty"] = specialty if present
   - Note: filtering requires re-index (upsert) for existing points to carry the new payload field.

3) Runtime retrieval filter
   - Extend retrieve_items(query, top_k, doc_id, version, specialty):
     - if specialty provided, add must condition on payload field "specialty" == value
     - if doc_id also provided, both must conditions apply
   - Extend request models/endpoints:
     - POST /rag/query: accept optional specialty (string)
     - POST /rag/answer: accept optional specialty (string)
     - POST /rag/outline: accept optional specialty (string)
     - POST /rag/slides/plan: accept optional specialty (string) and propagate to outline/retrieval
   - Default behavior unchanged when specialty is omitted.

4) Slide plan propagation
   - /rag/slides/plan:
     - query path: pass specialty into outline generation, which passes it into retrieval
     - outline-provided path: no retrieval is done unless include_retrieved is true; in that case ensure retrieved items came from the same specialty (enforced by retrieval filter)

5) Test dataset expansion (GCS layout)
- Prepare at least 2 PDFs per specialty:
  raw/
    endodonzia/
    implantologia/
    conservativa/
- For each doc:
  - Register with --specialty (or rely on path derivation)
  - Run DocAI ingest for a version (e.g. v1-docai)
  - Index into Qdrant for that version

Smoke tests (local)
- Index (repeat per doc_id):
  GCP_PROJECT=... GCP_LOCATION=... VERTEX_EMBED_MODEL=text-embedding-004 QDRANT_URL=http://localhost:6333 SQLITE_PATH=catalog.sqlite3 PYTHONPATH=src \
  python scripts/dev_index_items_qdrant.py --doc-id <doc_id> --version-id v1-docai

- Retrieval without filter:
  curl -s -X POST http://localhost:8000/rag/query -H "Content-Type: application/json" \
    -d '{"query":"...", "version_id":"v1-docai", "top_k":5}' | jq

- Retrieval with specialty filter:
  curl -s -X POST http://localhost:8000/rag/query -H "Content-Type: application/json" \
    -d '{"query":"...", "version_id":"v1-docai", "top_k":5, "specialty":"endodonzia"}' | jq

- Slides plan with specialty filter (query path):
  curl -s -X POST http://localhost:8000/rag/slides/plan -H "Content-Type: application/json" \
    -d '{"query":"...", "version":"v1-docai", "specialty":"implantologia"}' | jq

Operational notes
- Keep env vars explicit; no implicit defaults beyond existing behavior.
- No git push without explicit confirmation.
- Specialty filtering requires payload presence; re-index older docs after adding the payload field.

================================================
UPDATE LOG — v0.14 (APPEND-ONLY)
================================================
Date: 2026-02-13 (Europe/Rome)

GCS RAW ARCHITECTURE — Specialties folder convention (EN)
--------------------------------------------------------
Decision
- Macro-specialty folder names will be ENGLISH to match: (a) majority of raw docs (80–90% EN) and (b) slide output language (EN).
- Specialty values are stable identifiers: lowercase + underscores only.

Bucket layout
- Raw bucket: gs://odonto-raw-docs-matcler/raw/<specialty>/<file>.pdf
- Parsed bucket remains separate: gs://odonto-parsed-matcler/<doc_id>/<version_id>/...

MVP specialties (initial folders)
- endodontics
- restorative
- implantology
- zygomatic_implants
- periodontology
- oral_surgery
- prosthodontics
- orthodontics
- pediatric_dentistry

Operational note
- Adding new specialties later is supported (no schema changes).
- Renaming an existing specialty requires re-indexing points in Qdrant for affected docs.

NEXT IMPLEMENTATION STEPS (S1 execution checklist)
--------------------------------------------------
1) Update dev_register_doc_version.py
   - add --specialty
   - derive specialty from raw URI (raw/<specialty>/...) if flag missing
   - persist to documents.metadata_json["specialty"] without overwriting existing keys (e.g., "citation")

2) Update dev_index_items_qdrant.py
   - read documents.metadata_json["specialty"] once per doc_id
   - add payload["specialty"] to each point if present
   - re-index existing docs to backfill payload specialty field

3) Update API retrieval + slide plan propagation
   - add optional specialty to request models (/rag/query, /rag/answer, /rag/outline, /rag/slides/plan)
   - apply Qdrant filter key="specialty" when provided, combined with doc_id filter if present
   - ensure slides plan passes specialty through to retrieval/outline

4) Smoke tests
   - register + ingest + index 3 implantology docs
   - confirm: /rag/query with specialty="implantology" returns only implantology docs
   - confirm: /rag/query with specialty="endodontics" returns none (until docs added)
   - confirm: /rag/slides/plan specialty-scoped does not mix sources

================================================
UPDATE LOG — v0.15 (APPEND-ONLY)
================================================
Date: 2026-02-16 (Europe/Rome)

STEP S1 — Macro-specialty segmentation (SMOKE TEST END-TO-END)
--------------------------------------------------------------
Status: ✅ PASSED (implantology-only corpus)

Key runtime fixes (DocAI EU)
- Confirmed processor:
  - displayName: odonto-layout-parser
  - type: Layout Parser
  - region: eu
  - processor_id: cefc5e7bf97d1e2c
  - prediction endpoint: https://eu-documentai.googleapis.com/.../locations/eu/processors/<id>:process
- Root cause of prior failures:
  - Using global endpoint documentai.googleapis.com caused deployment mismatch and INVALID_ARGUMENT for EU processors.
- Fix implemented:
  - Use EU regional endpoint for both:
    (a) REST “discovery” checks and (b) gRPC client api_endpoint.
  - Verified via log:
    [docai] api_endpoint=eu-documentai.googleapis.com location=eu ...

Parsed bucket path convention (confirmed)
- Parsed layout outputs are stored under an explicit "parsed/" prefix:
  gs://odonto-parsed-matcler/parsed/<doc_id>/<version_id>/
  Files include: items.jsonl, assets.jsonl, plus assets/ and raw/ subfolders.

Ingest validation (EU)
- implant-macrodesign-osseo:
  - OK ingested via Document AI (gsutil)
  - items: 268, assets: 6
  - GCS: gs://odonto-parsed-matcler/parsed/implant-macrodesign-osseo/v1-docai/...
- implant-lovatto-2018:
  - OK ingested via Document AI (gsutil)
  - items: 249, assets: 4
  - GCS: gs://odonto-parsed-matcler/parsed/implant-lovatto-2018/v1-docai/...

Qdrant indexing validation
- Indexed and counted per doc_id matched expected items:
  - implant-macrodesign-10y: expected_n_items=231, qdrant_count=231
  - implant-macrodesign-osseo: expected_n_items=268, qdrant_count=268
  - implant-lovatto-2018: expected_n_items=249, qdrant_count=249

Retrieval filter smoke test
- POST /rag/query with specialty="implantology": ✅ non-empty results (top_k=5)
- POST /rag/query with specialty="endodontics": ✅ results=[] (expected because no endodontics docs ingested yet)
Interpretation:
- Specialty filter is functioning and prevents cross-specialty bleed.

Local API runtime note
- To complete smoke, FastAPI/Uvicorn were installed and API run locally.

Implementation note (index script)
- scripts/dev_index_items_qdrant.py was updated during smoke to:
  - ensure Qdrant point IDs are valid UUIDs
  - add sanity checks for doc_id/version_id

NEXT STEPS (S1 hardening)
-------------------------
1) Commit-level review
- Capture the exact diffs for:
  - scripts/dev_docai_layout_ingest.py (EU endpoint selection + discovery)
  - scripts/dev_index_items_qdrant.py (UUID ids + sanity checks + specialty payload)
  - src/odonto_rag/api/rag_app.py (specialty propagation + filters)
  - requirements / dependency notes (fastapi/uvicorn if now required)

2) Add a minimal regression test checklist
- One doc in endodontics + rerun the same smoke to confirm:
  - specialty=endodontics returns only endodontics
  - specialty=implantology returns only implantology
  - specialty omitted searches across both

3) Slides plan scoped test
- Run /rag/slides/plan with specialty="implantology" and verify:
  - citations come only from implantology docs
  - no bleed once additional specialties are added

Workflow rules reminder
- No git push without explicit confirmation.
- Master remains append-only; updates delivered as downloadable .txt.

